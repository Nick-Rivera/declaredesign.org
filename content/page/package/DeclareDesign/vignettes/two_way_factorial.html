---
title: "Two-Way Factorial Experiment"
output: rmarkdown::html_vignette
bibliography: bib.bib
vignette: >
  %\VignetteIndexEntry{Two-Way Factorial Experiment}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---



<div class="btn-group">
<p><a class="btn btn-primary" href="http://declaredesign.org/articles/two_way_factorial.R">
<i class="fa fa-code" title = "Download code for design" fa-2x></i> Download code
</a>
<a class="btn btn-primary" href="http://declaredesign.org/articles/two_way_factorial_template.RDS">
<i class="fa fa-clipboard" title = "Download template for design" fa-2x></i> Download template
</a>
<!--
  <a class="btn btn-primary" href="http://shiny.declaredesign.org/inspector/?topic=two_way_factorial">
    <i class="fa fa-area-chart" title = "Go to the design inspector" fa-2x></i> Inspect design
  </a>
--></p>
</div>
<p>Often we wish to know whether the effect of some causal factor increases or decreases in the presence of another causal factor. For example, when seeking to increase voter turnout, is the effect of phone calls larger or smaller when voters have already received mail bearing the same message? If the message is made more convincing through repetition then we may expect phone calls increase turnout more effectively when voters have already received mail. However, a phone call might also be less effective when someone has already received mail, because the person becomes over-saturated with messages (<span class="citation">Green, McGrath, and Aronow (2013)</span>).</p>
<p>When the effect of <span class="math inline">\(A\)</span> is bigger in the presence of <span class="math inline">\(B\)</span> we call this a positive interaction, and when it is smaller in the presence of <span class="math inline">\(B\)</span> we call it a negative interaction. Note that the direction of the interaction depends on the difference in the effects. Suppose, for example, that the effect of <span class="math inline">\(A\)</span> when <span class="math inline">\(B\)</span> is absent is -10 and when <span class="math inline">\(B\)</span> is present it is -7. Even though these effects are negative, the interaction is positive because the presence of <span class="math inline">\(B\)</span> increases the effect of <span class="math inline">\(A\)</span> by 3.</p>
<p>The most straightforward way to investigate interactions is through the two-way factorial design. Participants in a study are assigned – usually in equal proportions – to one of four conditions: 1) neither <span class="math inline">\(A\)</span> nor <span class="math inline">\(B\)</span>; 2) <span class="math inline">\(A\)</span> only; 3) <span class="math inline">\(B\)</span> only; or, 4) both <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span>. Those assigned to condition 1 don’t receive any effects, those assigned to 2 experience the effects of <span class="math inline">\(A\)</span> only, those assigned to 3 receive the effects of <span class="math inline">\(B\)</span> only, and those assigned to 4 receive both the effects of <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span>, as well as whatever extra effect the combination of <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> produce (the interaction). To isolate the interaction, we must firstly use the groups 1 and 2 to estimate the ‘pure’ effect of <span class="math inline">\(A\)</span> and groups 1 and 3 to estimate the ‘pure’ effect of <span class="math inline">\(B\)</span>. When we subtract these two estimates from group 4, we are left with the interaction.</p>
<p>As may be clear from this description, however, estimating all of these quantities at once requires a lot of data to achieve statistical precision. Exactly how big does a researcher’s design have to be in order to have at least 80% probability of rejecting the null hypothesis of no effect for a given effect size? This is a surprisingly difficult question to answer, because answering it accurately depends upon the specific model of the potential outcomes the researcher has in mind. As a result, many researchers use rules of thumb that may lead to systematic over- or under-confidence. Using the MIDA framework and design simulation, however, we can provide a flexible answer to this question that does not rely on rules of thumb.</p>
<div id="design-declaration" class="section level2">
<h2>Design Declaration</h2>
<ul>
<li><p><strong>M</strong>odel:</p>
<p>We specify a population of <span class="math inline">\(N\)</span> study participants, each of whom has four potential outcomes corresponding to the 4 conditions to which they could be assigned, as enumerated previously. When they are in the first condition, their outcome <span class="math inline">\(Y_0\)</span> is standard normally distributed baseline noise. In the second, the potential outcome <span class="math inline">\(Y_A\)</span> is their baseline plus the effect of <span class="math inline">\(A\)</span>, and in the third <span class="math inline">\(Y_B\)</span> is their baseline plus the effect of <span class="math inline">\(B\)</span>. In the fourth condition they reveal the potential outcome <span class="math inline">\(Y_{AB}\)</span>, receiving the <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> effects, as well as the supplementary effect produced by the combination of <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> – the interaction. In our simulations below we set <span class="math inline">\(N = 1000\)</span>, <span class="math inline">\(\beta_A = \beta_B = 0\)</span> and <span class="math inline">\(\beta_{AB} = .25\)</span>. This amounts to an assumption that both causes must be present in order to produce an effect, and that the effect thereby produced is equal to a quarter of a standard deviation in the outcome.</p></li>
<li><p><strong>I</strong>nquiry:</p>
<p>We wish to know the true average interaction, <span class="math inline">\(\beta_{AB}\)</span>. Note <span class="math inline">\(Y_{AB} - Y_B = \beta_{AB} + \beta_A\)</span>, and <span class="math inline">\(Y_{A} - Y_{0} = \beta_A\)</span>. Thus, the expected difference of these terms is equal to the interaction: <span class="math inline">\(E[(Y_{AB} - Y_B) - (Y_A - Y_0)] = \beta_{AB}\)</span>.</p></li>
<li><p><strong>D</strong>ata strategy:</p>
<p>We assign equal proportions of the participants to each of the four combinations of treatment.</p></li>
<li><p><strong>A</strong>nswer strategy:</p>
<p>We estimate the interaction by regressing the outcome on indicators for the two treatments and their interaction.</p></li>
</ul>
<pre class="r"><code># Set design parameters --------------------------------------------------------
N &lt;- 1000
beta_A &lt;- 0
beta_B &lt;- 0
beta_AB &lt;- .25

# Model ------------------------------------------------------------------------
population &lt;- declare_population(N = N, noise = rnorm(N))

potential_outcomes &lt;- declare_potential_outcomes(
  Y_Z_T1 = noise,
  Y_Z_T2 = noise + beta_A,
  Y_Z_T3 = noise + beta_B,
  Y_Z_T4 = noise + beta_A + beta_B + beta_AB)

# Inquiry ----------------------------------------------------------------------
estimand &lt;- declare_estimand(
  interaction = mean((Y_Z_T4 - Y_Z_T3) - (Y_Z_T2 - Y_Z_T1)), 
  label = &quot;interaction&quot;)

# Data Strategy ----------------------------------------------------------------
assignment &lt;- declare_assignment(num_arms = 4)

# Answer Strategy --------------------------------------------------------------
estimator &lt;- declare_estimator(Y ~ A + B + A:B,
                               model = lm_robust,
                               coefficient_name = &quot;A:B&quot;, 
                               estimand = estimand)

# Design -----------------------------------------------------------------------
design &lt;- declare_design(
  population,
  potential_outcomes,
  estimand,
  assignment,
  dplyr::mutate(A = as.numeric(Z %in% c(&quot;T2&quot;, &quot;T4&quot;)),
                B = as.numeric(Z %in% c(&quot;T3&quot;, &quot;T4&quot;))),
  reveal_outcomes,
  estimator)</code></pre>
</div>
<div id="takeaways" class="section level2">
<h2>Takeaways</h2>
<p>With the design declared we can run a diagnosis from Monte Carlo simulations of the design:</p>
<pre class="r"><code>diagnosis &lt;- diagnose_design(design, sims = 10000, bootstrap_sims = 1000)</code></pre>
<table>
<thead>
<tr class="header">
<th align="right">Mean Estimate</th>
<th align="right">Mean Estimand</th>
<th align="right">Bias</th>
<th align="right">SE(bias)</th>
<th align="right">Power</th>
<th align="right">SE(Power)</th>
<th align="right">Coverage</th>
<th align="right">SE(Coverage)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">0.25</td>
<td align="right">0.25</td>
<td align="right">0</td>
<td align="right">0.001</td>
<td align="right">0.5</td>
<td align="right">0.005</td>
<td align="right">0.951</td>
<td align="right">0.002</td>
</tr>
</tbody>
</table>
<p>The first thing to note about the design is that it produces an unbiased estimate of the quantity we care about: the average estimate is equal to the average estimand, at 0.25. The design exhibits good coverage too, with the estimand falling within the estimated confidence intervals roughly 95% of the time.</p>
<p>Note, however, that the power of this design is very low. Even with 1000 subjects, at 50% power, whether or not we detect the reasonably large one-quarter standard deviation effect is essentially a coin toss. Half of the time we run the risk of erroneously maintaining a null of no effect, whereas in combination the treatments do in fact produce a fairly large effect.</p>
<p>We can use the design template <code>two_way_factorial_template</code> to quickly declare an array of designs that increment the <span class="math inline">\(N\)</span> from 150 to 3,000 by increments of 150. Diagnosing and plotting the results of this exercise reveals that we would require over 2,000 respondents to reach statistical power at the conventional 80% level.</p>
<pre class="r"><code>designs &lt;- quick_design(two_way_factorial_template,
                        N = seq(150,3000,150),  
                        beta_A = 0,
                        beta_B = 0,
                        beta_AB = .25)
diagnoses_N &lt;- diagnose_design(designs, sims = 1000, bootstrap = FALSE)</code></pre>
<p><img src="/page/package/DeclareDesign/vignettes/two_way_factorial_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
</div>
<div id="further-reading" class="section level2">
<h2>Further Reading</h2>
<p>Factorial designs are used in all kinds of diverse contexts, consider these two for example:</p>
<ul>
<li><p>See <span class="citation">Blattman, Jamison, and Sheridan (2017)</span> for a factorial design that sought to test the theory that giving cash to criminally-engaged men in Uganda would produce a bigger reduction in their subsequent criminal behavior if it was coupled with cognitive behavioral therapy. The authors reasoned that cash windfalls would be especially likely to shift those with high business ability to non-criminal careers where those abilities were comparatively more valuable, and that therapy would increase business ability.</p></li>
<li><p><span class="citation">Kalla and Aronow (2015)</span> used a factorial design to study bias in the way that political information is published and edited on Wikipedia. They randomly assigned the web pages of 251 US Senators to have one of four conditions: a positive fact added to the article, with a citation; a negative fact added, with citation; a positive without citation and finally a negative without citation.</p></li>
</ul>
</div>
<div id="exercises" class="section level2">
<h2>Exercises</h2>
<ol style="list-style-type: decimal">
<li><p>Alter the template so that outcomes are binary instead of normally distributed. What is the expected standard error for the interaction term for a sample size of 1000? Discuss the implications of your diagnosis for practice.</p></li>
<li><p>Assess how the power of the study to detect the interaction term varies as a function of the size and direction of the main effects of <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span>.</p></li>
<li><p>Find the minimal sample size needed to achieve 90% power with an interaction effect of 1/10th of a standard deviation, setting the main effects to zero.</p></li>
</ol>
</div>
<div id="references" class="section level2 unnumbered">
<h2>References</h2>
<div id="refs" class="references">
<div id="ref-blattman2017reducing">
<p>Blattman, Christopher, Julian C Jamison, and Margaret Sheridan. 2017. “Reducing Crime and Violence: Experimental Evidence from Cognitive Behavioral Therapy in Liberia.” <em>The American Economic Review</em> 107 (4). American Economic Association:1165–1206.</p>
</div>
<div id="ref-green2013field">
<p>Green, Donald P, Mary C McGrath, and Peter M Aronow. 2013. “Field Experiments and the Study of Voter Turnout.” <em>Journal of Elections, Public Opinion &amp; Parties</em> 23 (1). Taylor &amp; Francis:27–48.</p>
</div>
<div id="ref-kalla2015editorial">
<p>Kalla, Joshua L, and Peter M Aronow. 2015. “Editorial Bias in Crowd-Sourced Political Information.” <em>PloS One</em> 10 (9). Public Library of Science:e0136327.</p>
</div>
</div>
</div>
