---
title: "How Stata's hat matrix differs with weights"
author: "Luke Sonnet"
output:
  html_document:
    df_print: paged
link-citations: yes
bibliography: estimatr.bib
vignette: |
  %\VignetteIndexEntry{How Stata's hat matrix differs with weights} 
  \usepackage[utf8]{inputenc}
  %\VignetteEngine{knitr::rmarkdown}
editor_options: 
  chunk_output_type: console
---



<p>Researchers use linear regression with heteroskedasticity-robust standard errors. Many social scientists use either Stata or R. One would hope the two would always agree in their estimates. Unfortunately, estimating weighted least squares with HC2 or HC3 robust variance results in different answers across Stata and common approaches in R as well as Python.</p>
<p>The discrepancy is due to differences in how the software estimates the “hat” matrix, on which both HC2 and HC3 variance estimators rely. The short story is that Stata estimates the hat matrix as</p>
<p><span class="math display">\[
\mathbf{H} = \mathbf{X} (\mathbf{X}^{\top}\mathbf{W}\mathbf{X})^{-1} \mathbf{X}^\top
\]</span></p>
<p>while the usual approaches in R, including <a href="https://CRAN.R-project.org/package=sandwich"><code>sandwich</code></a> and <a href="estimatr.declaredesign.org"><code>estimatr</code></a>, and Python (e.g. <a href="http://www.statsmodels.org/stable/index.html"><code>statsmodels</code></a>) estimate the following hat matrix</p>
<p><span class="math display">\[
\mathbf{H} = \mathbf{X} (\mathbf{X}^{\top}\mathbf{W}\mathbf{X})^{-1} \mathbf{X}^\top \mathbf{W}
\]</span></p>
<p>This results in differences when researches estimate HC2 and HC3 variance estimators. The HC1 standard errors, Stata’s default, are the same across all packages. The rest of this document just walks through the set-up for the above and demonstrates some results from Stata, R, and Python.</p>
<div id="weighted-least-squares" class="section level2">
<h2>Weighted least squares</h2>
<p>Let’s briefly review WLS. Weights are used in linear regression often for two key problems; (1) to model and correct for heteroskedasticity, and (2) to deal with unequal sampling (or treatment) probabilities. In both cases, we take the standard model</p>
<p><span class="math display">\[
y_i = \mathbf{x}_i^\top \mathbf{\beta} + \epsilon_i,
\]</span></p>
<p>where <span class="math inline">\(y_i\)</span> is the <span class="math inline">\(i\)</span>th unit’s outcome, <span class="math inline">\(\mathbf{x}_i\)</span> is a column vector of covariates, <span class="math inline">\(\mathbf{\beta}\)</span> are the coefficients of interest, and <span class="math inline">\(\epsilon\)</span> is some error, and rescale the model by the square root of that unit’s weight, <span class="math inline">\(\sqrt{w_i}\)</span>. Our model then becomes</p>
<p><span class="math display">\[
\frac{y_i}{\sqrt{w_i}} = \frac{\mathbf{x}_i^\top}{\sqrt{w_i}} \mathbf{\beta} + \frac{\epsilon_i}{\sqrt{w_i}}.
\]</span></p>
<p>It can be shown that the solution for <span class="math inline">\(\mathbf{\beta}\)</span> is</p>
<p><span class="math display">\[
\widehat{\mathbf{\beta}} = (\mathbf{X}^{\top}\mathbf{W}\mathbf{X})^{-1} \mathbf{X}^\top \mathbf{W} \mathbf{y},
\]</span></p>
<p>where <span class="math inline">\(\mathbf{W}\)</span> is a diagonal matrix where each entry is <span class="math inline">\(w_{i}\)</span>, <span class="math inline">\(\mathbf{X}\)</span> is the covariate matrix, and <span class="math inline">\(\mathbf{y}\)</span> is the outcome column vector. Note that all weights have been scaled to sum to 1 (i.e., <span class="math inline">\(\sum_i w_{ii} = 1\)</span>). An easy way to get to compute <span class="math inline">\(\widehat{\mathbf{\beta}}\)</span> is to first weight both <span class="math inline">\(\mathbf{X}\)</span> and <span class="math inline">\(\mathbf{y}\)</span> by <span class="math inline">\(\mathbf{W}^s\)</span>, which is simply the weight matrix but using instead the square root of the weights. Let’s define these rescaled matrices as</p>
<p><span class="math display">\[
\begin{aligned}
\widetilde{\mathbf{X}} &amp;= \mathbf{X} \mathbf{W}^s \\
\widetilde{\mathbf{y}} &amp;= \mathbf{W}^s \mathbf{y} 
\end{aligned}
\]</span></p>
</div>
<div id="heteroskedastic-consistent-variance-estimators" class="section level2">
<h2>Heteroskedastic-consistent variance estimators</h2>
<p>Turning to variance, the standard sandwich estimator is</p>
<p><span class="math display">\[
\mathbb{V}[\widehat{\mathbf{\beta}}] = (\mathbf{X}^{\top}\mathbf{X})^{-1} \mathbf{X}^\top \Omega \mathbf{X} (\mathbf{X}^{\top}\mathbf{X})^{-1} 
\]</span></p>
<p>where <span class="math inline">\(\Omega\)</span> represents <span class="math inline">\(\mathbb{E}[\mathbf{\epsilon}\mathbf{\epsilon}^\top]\)</span>, the variance-covariance matrix of the disturbances. A nice review of the different variance estimators along with their properties can be found in <span class="citation">Long and Ervin (<a href="#ref-longervin2000">2000</a>)</span> <a href="http://www.indiana.edu/~jslsoc/files_research/testing_tests/hccm/99TAS.pdf">[ungated]</a>. The HC2 and HC3 estimators, introduced by <span class="citation">MacKinnon and White (<a href="#ref-mackinnonwhite1985">1985</a>)</span>, use the hat matrix as part of the estimation of <span class="math inline">\(\Omega\)</span>. The standard hat matrix is written:</p>
<p><span class="math display">\[
\mathbf{H} = \mathbf{X} (\mathbf{X}^{\top}\mathbf{X})^{-1} \mathbf{X}^\top
\]</span></p>
<p>Where <span class="math inline">\(h_{ii}\)</span> are the diagonal elements of the hat matrix, the HC2 variance estimator is</p>
<p><span class="math display">\[
\mathbb{V}[\widehat{\mathbf{\beta}}]_{HC2} = (\mathbf{X}^{\top}\mathbf{X})^{-1} \mathbf{X}^\top \mathrm{diag}\left[\frac{e^2_i}{1 - h_{ii}}\right] \mathbf{X} (\mathbf{X}^{\top}\mathbf{X})^{-1}  ,
\]</span></p>
<p>where <span class="math inline">\(e_i\)</span> are the residuals. The HC3 estimator is very similar,</p>
<p><span class="math display">\[
\mathbb{V}[\widehat{\mathbf{\beta}}]_{HC2} = (\mathbf{X}^{\top}\mathbf{X})^{-1} \mathbf{X}^\top \mathrm{diag}\left[\frac{e^2_i}{1 - (h_{ii})^2}\right] \mathbf{X} (\mathbf{X}^{\top}\mathbf{X})^{-1} .
\]</span></p>
<p>Both rely on the hat matrix. Crucially, this is where Stata and the packages and modules in R and Python disagree. When weights are specified, Stata estimates the hat matrix as</p>
<p><span class="math display">\[
\mathbf{H}_{Stata} = \mathbf{X} (\mathbf{X}^{\top}\mathbf{W}\mathbf{X})^{-1} \mathbf{X}^\top,
\]</span></p>
<p>while the other software uses</p>
<p><span class="math display">\[
\mathbf{H}_{R} = \mathbf{X} (\mathbf{X}^{\top}\mathbf{W}\mathbf{X})^{-1} \mathbf{X}^\top \mathbf{W}.
\]</span></p>
<p>Thus the HC2 and HC3 estimator differ as the values of <span class="math inline">\(h_{ii}\)</span> are quite different. How different are these results? Let’s use a little example using <code>mtcars</code> a dataset included with R.</p>
<pre class="r"><code># Using estimatr
library(estimatr)
lm_robust(
  mpg ~ hp, 
  data = mtcars, 
  weights = wt, 
  se_type = &quot;HC2&quot;
)
#&gt;                Estimate Std. Error     Pr(&gt;|t|)    CI Lower    CI Upper DF
#&gt; (Intercept) 28.54864505 2.16281844 4.975934e-14 24.13158053 32.96570958 30
#&gt; hp          -0.06249413 0.01445662 1.561752e-04 -0.09201849 -0.03296977 30</code></pre>
<p>We can also see that Python’s <a href="http://www.statsmodels.org/stable/index.html"><code>statsmodels</code></a> provides the same results as the methods in R (and in fact they note the difference in an <a href="https://github.com/statsmodels/statsmodels/issues/1209">issue on GitHub</a>).</p>
<pre class="python"><code>import statsmodels.api as sm
import pandas as pd
dat = pd.read_csv(&#39;mtcars.csv&#39;)
wls_mod = sm.WLS(dat[&#39;mpg&#39;], sm.add_constant(dat[&#39;hp&#39;]), weights = dat[&#39;wt&#39;])
print(wls_mod.fit().HC2_se)
#&gt; const    2.162818
#&gt; hp       0.014457
#&gt; dtype: float64</code></pre>
<p>If we do the same in Stata 13, we get the following output:</p>
<pre class="stata"><code>insheet using mtcars.csv
reg mpg hp [aweight=wt], vce(hc2)</code></pre>
<pre><code>Linear regression                                      Number of obs =      32
                                                       F(  1,    30) =   19.08
                                                       Prob &gt; F      =  0.0001
                                                       R-squared     =  0.5851
                                                       Root MSE      =  3.6191

------------------------------------------------------------------------------
             |             Robust HC2
         mpg |      Coef.   Std. Err.      t    P&gt;|t|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
          hp |  -.0624941   .0143083    -4.37   0.000    -.0917155   -.0332727
       _cons |   28.54865   2.155169    13.25   0.000      24.1472    32.95009
------------------------------------------------------------------------------</code></pre>
<p>Stata’s standard errors are somewhat different. The only documentation of Stata’s formula for the hat matrix can be found on the <a href="https://www.statalist.org/forums/forum/general-stata-discussion/general/329653-regress-postestimation-with-weights">statalist forum here</a> and nowhere in the official documentation as far as I can tell.</p>
<div id="which-should-we-prefer" class="section level4">
<h4>Which should we prefer?</h4>
<p>Just because Stata is not documenting their HC2 and HC3 estimator does not mean they’re wrong. Also the differences tend to be minor. In fact, it is unclear which we should prefer given that there is not a strong literature supporting one or the other. However, there are several arguments to be made for <span class="math inline">\(\matbf{H}_{R}\)</span>.</p>
<ol style="list-style-type: decimal">
<li>It’s the estimator you get when you weight your data by the square root of the weights (<span class="math inline">\(\mathbf{X} \rightarrow \widetilde{\mathbf{X}}\)</span> and <span class="math inline">\(\mathbf{y} \rightarrow \widetilde{\mathbf{y}}\)</span>) and fit regular ordinary least squares. If one considers the weighted model as simply a rescaled version of the unweighted moded, then users should prefer <span class="math inline">\(\mathbf{H}_{R}\)</span>.</li>
<li>The diagonal of <span class="math inline">\(\mathbf{H}_{R}\)</span> are the weighted leverages <span class="citation">(Li and Valliant <a href="#ref-livalliant2009">2009</a>)</span>, while <span class="math inline">\(\mathbf{H}_{Stata}\)</span> would need to be weighted again for the diagonal to recover the weighted leverage.</li>
</ol>
</div>
</div>
<div id="references" class="section level2 unnumbered">
<h2>References</h2>
<div id="refs" class="references">
<div id="ref-livalliant2009">
<p>Li, Jianzhu, and Richard Valliant. 2009. “Survey Weighted Hat Matrix and Leverages.” <em>Survey Methodology</em> 35 (1). Statistics Canada:15–24.</p>
</div>
<div id="ref-longervin2000">
<p>Long, J Scott, and Laurie H Ervin. 2000. “Using Heteroscedasticity Consistent Standard Errors in the Linear Regression Model.” <em>The American Statistician</em> 54 (3). Taylor &amp; Francis Group:217–24. <a href="https://doi.org/10.1080/00031305.2000.10474549" class="uri">https://doi.org/10.1080/00031305.2000.10474549</a>.</p>
</div>
<div id="ref-mackinnonwhite1985">
<p>MacKinnon, James, and Halbert White. 1985. “Some Heteroskedasticity-Consistent Covariance Matrix Estimators with Improved Finite Sample Properties.” <em>Journal of Econometrics</em> 29 (3):305–25. <a href="https://doi.org/10.1016/0304-4076(85)90158-7" class="uri">https://doi.org/10.1016/0304-4076(85)90158-7</a>.</p>
</div>
</div>
</div>
