---
title: "Benchmarking estimatr"
author: "Luke Sonnet"
output:
  html_document:
    df_print: paged
vignette: |
  %\VignetteIndexEntry{Benchmarking estimatr}
  %\VignetteEngine{knitr::knitr}
  \usepackage[utf8]{inputenc}
---



<p>We built <code>estimatr</code> to provide accurate standard errors <strong>quickly</strong>.
This document benchmarks the speed of or linear regression estimator
against other estimators. Our performance is slightly better than base R
when using classical standard errors, but most of our improvements come
when estimating robust standard errors.</p>
<p>Furthermore, we provide an option in our <code>lm_robust()</code> and <code>lm_lin()</code>
estimators, <code>try_cholesky</code>, which users should set to <code>TRUE</code> if they are
concerned about speed and are certain their analysis does not suffer
from perfect multicollinearity (linear dependencies).</p>
<div id="linear-regression" class="section level1">
<h1>Linear regression</h1>
<p>I test our speed in estimating coefficients, standard errors, and doing
inference on four different datasets (500 and 5000 observations; 5 and
50 covariates) and across several different specifications. Below I
preview the results comparing <code>lm_robust()</code> to base R for fitting
coefficients and a commonly used package for robust standard errors,
such as the <code>sandwich</code> package. In the two largest datasets, our method
is almost always faster and at worst is the same as base R, and only
with classical standard errors. When it comes to the biggest gains,
using <code>lm_robust()</code> to get HC2 or Stata-like cluster-robust standard
errors will roughly halve your waiting time. If you want CR2 standard
errors, <code>lm_robust()</code> can reduce your run time by a factor of 10!</p>
<table>
<thead>
<tr class="header">
<th>
N. Obs
</th>
<th>
N. Coefs
</th>
<th>
Estimator
</th>
<th>
Classical SEs
</th>
<th>
HC2 SEs
</th>
<th>
Stata clustered SEs
</th>
<th>
CR2 SEs
</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>
500
</td>
<td>
5
</td>
<td>
<code>estimatr::lm_robust()</code>
</td>
<td>
1.9
</td>
<td>
<strong>2.3</strong>
</td>
<td>
<strong>2</strong>
</td>
<td>
<strong>6</strong>
</td>
</tr>
<tr class="even">
<td>
</td>
<td>
</td>
<td>
base + sandwich/clubSandwich
</td>
<td>
<strong>1.7</strong>
</td>
<td>
5.2
</td>
<td>
4.4
</td>
<td>
66
</td>
</tr>
<tr class="odd">
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr class="even">
<td>
5000
</td>
<td>
5
</td>
<td>
<code>estimatr::lm_robust()</code>
</td>
<td>
<strong>4.6</strong>
</td>
<td>
<strong>7.9</strong>
</td>
<td>
<strong>7.8</strong>
</td>
<td>
<strong>172</strong>
</td>
</tr>
<tr class="odd">
<td>
</td>
<td>
</td>
<td>
base + sandwich/clubSandwich
</td>
<td>
<strong>4.6</strong>
</td>
<td>
22.4
</td>
<td>
21.7
</td>
<td>
2268
</td>
</tr>
<tr class="even">
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr class="odd">
<td>
500
</td>
<td>
50
</td>
<td>
<code>estimatr::lm_robust()</code>
</td>
<td>
<strong>5.8</strong>
</td>
<td>
<strong>8.2</strong>
</td>
<td>
<strong>8.2</strong>
</td>
<td>
<strong>62</strong>
</td>
</tr>
<tr class="even">
<td>
</td>
<td>
</td>
<td>
base + sandwich/clubSandwich
</td>
<td>
6.7
</td>
<td>
20.2
</td>
<td>
29.2
</td>
<td>
160
</td>
</tr>
<tr class="odd">
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr class="even">
<td>
5000
</td>
<td>
50
</td>
<td>
<code>estimatr::lm_robust()</code>
</td>
<td>
<strong>26.3</strong>
</td>
<td>
<strong>41.9</strong>
</td>
<td>
<strong>55</strong>
</td>
<td>
<strong>2504</strong>
</td>
</tr>
<tr class="odd">
<td>
</td>
<td>
</td>
<td>
base + sandwich/clubSandwich
</td>
<td>
32.2
</td>
<td>
114.8
</td>
<td>
253.8
</td>
<td>
10166
</td>
</tr>
</tbody>
</table>
<p>The times are milliseconds and are a median over 200 runs for all but
the CR2 case, which was taken on a sample of 50 runs, using the
<code>microbenchmark</code> package. This benchmarking was done on a 2017 MacBook
Air, with a 1.8 GHz Intel Core i5 CPU and 8 GB of memory.</p>
<p>To see the exact comparisons, see below.</p>
<pre><code>library(estimatr)
library(microbenchmark)
# Create some data sets of different sizes for testing below
set.seed(42)
data_size &lt;- expand.grid(list(ns = c(500, 5000), ps = c(5, 50)))
data_list &lt;- lapply(
  1:nrow(data_size), 
  function(i) {
    n &lt;- data_size$ns[i]
    p &lt;- data_size$ps[i]
    y &lt;- rnorm(n)
    X &lt;- matrix(rnorm(n*p), n, p)
    return(data.frame(y, X))
  }
)</code></pre>
<p>First I compare to a couple other methods of the classical standard
errors. First, let’s compare against base R, RcppEigen’s <code>fastLm()</code>
function (from which we borrow much of our algorithm), and
RcppArmadillo’s <code>fastLm()</code> function.</p>
<pre><code>library(RcppEigen)
library(RcppArmadillo)

test_base &lt;- lapply(data_list, function(dat) {
  mbo &lt;- summary(microbenchmark(
    &#39;lm_robust&#39; = lm_robust(y ~ ., data = dat, se_type = &quot;classical&quot;),
    &#39;base&#39; = summary(lm(y ~ ., data = dat)),
    &#39;RcppEigen&#39; = RcppEigen:::summary.fastLm(
      RcppEigen::fastLm(y ~ ., data = dat)
    ),
    &quot;RcppArmadillo&quot; = RcppArmadillo:::summary.fastLm(
      RcppArmadillo::fastLm(y ~ ., data = dat)
    ),
    times = 200L
  ),
  unit = &quot;ms&quot;)
  return(mbo[, c(&quot;expr&quot;, &quot;median&quot;)])
})</code></pre>
<p>The following table has the median time in milliseconds across 50 runs
of each estimator for each of the different data sets.</p>
<table>
<thead>
<tr class="header">
<th align="left">
Estimator
</th>
<th align="right">
N=500, P=5
</th>
<th align="right">
N=500, P=50
</th>
<th align="right">
N=5000, P=5
</th>
<th align="right">
N=500, P=50
</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">
lm_robust
</td>
<td align="right">
2
</td>
<td align="right">
5
</td>
<td align="right">
6
</td>
<td align="right">
26
</td>
</tr>
<tr class="even">
<td align="left">
base
</td>
<td align="right">
2
</td>
<td align="right">
5
</td>
<td align="right">
7
</td>
<td align="right">
32
</td>
</tr>
<tr class="odd">
<td align="left">
RcppEigen
</td>
<td align="right">
1
</td>
<td align="right">
5
</td>
<td align="right">
6
</td>
<td align="right">
32
</td>
</tr>
<tr class="even">
<td align="left">
RcppArmadillo
</td>
<td align="right">
2
</td>
<td align="right">
6
</td>
<td align="right">
10
</td>
<td align="right">
54
</td>
</tr>
</tbody>
</table>
<p>However, the real speed gains come with robust standard errors. Let’s
compare <code>lm_robust</code> to getting “HC2” standard errors and doing inference
using them from the <code>coeftest</code> and <code>sandwich</code> packages.</p>
<pre><code>library(sandwich)
library(lmtest)

test_rob &lt;- lapply(data_list, function(dat) {
  mbo &lt;- summary(microbenchmark(
    &#39;lm_robust&#39; = lm_robust(y ~ ., data = dat, se_type = &quot;HC2&quot;),
    &#39;lm + coeftest + sandwich&#39; = {
      lmo &lt;- lm(y ~ ., data = dat)
      coeftest(lmo, vcov = vcovHC(lmo, type = &quot;HC2&quot;))
    },
    times = 200L
  ),
  unit = &quot;ms&quot;)
  return(mbo[, c(&quot;expr&quot;, &quot;median&quot;)])
})</code></pre>
<table>
<thead>
<tr class="header">
<th align="left">
Estimator
</th>
<th align="right">
N=500, P=5
</th>
<th align="right">
N=500, P=50
</th>
<th align="right">
N=5000, P=5
</th>
<th align="right">
N=500, P=50
</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">
lm_robust
</td>
<td align="right">
2
</td>
<td align="right">
8
</td>
<td align="right">
8
</td>
<td align="right">
42
</td>
</tr>
<tr class="even">
<td align="left">
lm + coeftest + sandwich
</td>
<td align="right">
5
</td>
<td align="right">
22
</td>
<td align="right">
20
</td>
<td align="right">
115
</td>
</tr>
</tbody>
</table>
<p>What about with Stata’s clustered standard errors using <code>tapply</code> and
<code>sandwich</code>?</p>
<pre><code># Commonly used function attributed mostly to M. Arai replicating Stata 
# clustered SEs in R using sandwich and lmtest packages
cluster_robust_se &lt;- function(model, cluster){
  M &lt;- length(unique(cluster))
  N &lt;- length(cluster)
  K &lt;- model$rank
  dfc &lt;- (M/(M - 1)) * ((N - 1)/(N - K))
  uj &lt;- apply(estfun(model), 2, function(x) tapply(x, cluster, sum));
  rcse.cov &lt;- dfc * sandwich(model, meat = crossprod(uj)/N)
  rcse.se &lt;- coeftest(model, rcse.cov)
  return(list(rcse.cov, rcse.se))
}

test_cl &lt;- lapply(data_list, function(dat) {
  cluster &lt;- sample(nrow(dat)/5, size = nrow(dat), replace = TRUE)
  mbo &lt;- summary(microbenchmark(
    &#39;lm_robust&#39; = lm_robust(
      y ~ ., 
      data = dat, 
      clusters = cluster, 
      se_type = &quot;stata&quot;
    ),
    &#39;lm + coeftest + sandwich&#39; = {
      lmo &lt;- lm(y ~ ., data = dat)
      cluster_robust_se(lmo, cluster)
    },
    times = 200L
  ),
  unit = &quot;ms&quot;)
  return(mbo[, c(&quot;expr&quot;, &quot;median&quot;)])
})</code></pre>
<table>
<thead>
<tr class="header">
<th align="left">
Estimator
</th>
<th align="right">
N=500, P=5
</th>
<th align="right">
N=500, P=50
</th>
<th align="right">
N=5000, P=5
</th>
<th align="right">
N=500, P=50
</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">
lm_robust
</td>
<td align="right">
2
</td>
<td align="right">
8
</td>
<td align="right">
8
</td>
<td align="right">
55
</td>
</tr>
<tr class="even">
<td align="left">
lm + coeftest + sandwich
</td>
<td align="right">
4
</td>
<td align="right">
22
</td>
<td align="right">
29
</td>
<td align="right">
254
</td>
</tr>
</tbody>
</table>
<p>The original authors who came up with a generalized version of the CR2
errors and accompanying Satterthwaite-like corrected degrees of freedom
have their own package,
<a href="https://github.com/jepusto/clubSandwich"><code>clubSandwich</code></a>, that provides
estimators for many methods. We show here how much faster our
implementation is for simple linear regression.</p>
<pre><code>library(clubSandwich)

test_cr2 &lt;- lapply(data_list, function(dat) {
  cluster &lt;- sample(nrow(dat)/5, size = nrow(dat), replace = TRUE)
  mbo &lt;- summary(microbenchmark(
    &#39;lm_robust&#39; = lm_robust(
      y ~ ., 
      data = dat,
      clusters = cluster, 
      se_type = &quot;CR2&quot;
    ),
    &#39;lm + clubSandwich&#39; = {
      lmo &lt;- lm(y ~ ., data = dat)
      coef_test(lmo, vcov = vcovCR(lmo, cluster = cluster, type = &quot;CR2&quot;))
    },
    times = 50L
  ),
  unit = &quot;ms&quot;)
  return(mbo[, c(&quot;expr&quot;, &quot;median&quot;)])
})

knitr::kable(create_tab(test_cr2), col.names = col_names)</code></pre>
<table>
<thead>
<tr class="header">
<th align="left">
Estimator
</th>
<th align="right">
N=500, P=5
</th>
<th align="right">
N=500, P=50
</th>
<th align="right">
N=5000, P=5
</th>
<th align="right">
N=500, P=50
</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">
lm_robust
</td>
<td align="right">
6
</td>
<td align="right">
173
</td>
<td align="right">
62
</td>
<td align="right">
2504
</td>
</tr>
<tr class="even">
<td align="left">
lm + clubSandwich
</td>
<td align="right">
66
</td>
<td align="right">
2268
</td>
<td align="right">
160
</td>
<td align="right">
10166
</td>
</tr>
</tbody>
</table>
<pre><code>sessionInfo()
#&gt; R version 3.4.3 (2017-11-30)
#&gt; Platform: x86_64-apple-darwin14.5.0 (64-bit)
#&gt; Running under: OS X Yosemite 10.10.5
#&gt; 
#&gt; Matrix products: default
#&gt; BLAS: /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libBLAS.dylib
#&gt; LAPACK: /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libLAPACK.dylib
#&gt; 
#&gt; locale:
#&gt; [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
#&gt; 
#&gt; attached base packages:
#&gt; [1] stats     graphics  grDevices utils     datasets  methods   base     
#&gt; 
#&gt; loaded via a namespace (and not attached):
#&gt;  [1] compiler_3.4.3  backports_1.1.2 magrittr_1.5    rprojroot_1.3-2
#&gt;  [5] tools_3.4.3     htmltools_0.3.6 yaml_2.1.15     Rcpp_0.12.15   
#&gt;  [9] stringi_1.1.6   rmarkdown_1.8   highr_0.6       knitr_1.17     
#&gt; [13] stringr_1.2.0   digest_0.6.14   evaluate_0.10.1</code></pre>
</div>
